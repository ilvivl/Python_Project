# Zumamotu

## Участники проекта:
* Зубков Максим [github](https://github.com/maximzubkov)
* Турков Матвей [github](https://github.com/turk0v)
* Виноградов Илья [github](https://github.com/ilvivl)

## Описание проекта 
Мы хотим реализовать плагин для браузера, занимающийся анализом поведения пользователя, находящегося за компьютером. Мы будем собирать данные из браузера для как каждой web-страницы, так и для переходов по сайтам в целом, и с некоторым периодом обновлять модель, описывающее поведение пользователя. Модель будет строиться на основании каких-то показательных метрик полученных данных, такие как скорость движения мышки, скорость печатиб вероятность нахождения мыши в некотором конкретной области сайта и прочие. Важной особенностью является то, что мы будем проводить анализ только для тех сайтов, на которых пользователь аутентифицирован.

## Составные части проекта:
#### 0. Проверить предусмотрена ли на сайте аутентификация
#### 1. Создание плагина на google chrome, собирающего данные в формате json о поведении пользователя в сети
#### 2. Сбор данных о переходах пользователя между web-страницами планируется организовать граф переходов
#### 3. Связь сервера с клиентом, посредством python скрипта. Данные полученные в двух предыдущих пунктах будут в формате json отправлены на сервер и сам сервер будет заниматься распределением данных по БД и прочим
#### 4. Хранение полученных данных в БД на сервере
#### 5. Анализ данных поведения, который запускается сразу, как только необходимый объем данных потупил
#### 6. Анализ данных переходов по страницам

## Технологии и языки, используемые в работе

* Python
* JS, HTML, CSS
* PostgreSQL



## Детальное описание проекта
### 0. Проверить предусмотрена ли на сайте аутентификация.
### 1. Google chrome плагинами будет в основном будет заниматься Матвей. Вытаскивание данных из браузера будем при помощи JS. Рассмотрев информацию с данной [страницы](https://developer.mozilla.org/ru/docs/Web/Events), мы выделили несколько ключевых паттернов поведения пользователя(в скобках указано расположение данной информации в терминологии общей для всех расширений в браузерах):
- [x] Координаты, скорость и ускорение мыши (есть RAW данные, нужна обработка)(content_script)
- [x] Скорость печати, горячих клавиши (есть RAW данные, нужна обработка)(content_script)
- [x] Исправления в печати(content_script)
- [x] Навигация по сайту (через кнопки или с помощью колеса мыши)(content_script)
- [ ] Размер окна, скорость листания страниц (background)
- [ ] Количество открытых ссылок, количество закладок (background)
- [ ] Время проводимое на конкретных страницах сайта(content_script)
- [ ] Выбросы, то странные привычки, к примеру привычка листать страницу вверх-вниз, когда пользователь ожидает загрузки страницы(? вроде как должны быть выбросами в RAW данных)
- [ ] ping перед действием. Некоторые люди любят подумать, прежде чем нажать на ссылку, другие же жмут моментально(content_script HARD)
- [ ] Даблклик на объект(?)
- [ ] Количество запросов (background)
- [ ] Выделение текста, например, при чтении некоторым людям бывает удобно выделить мышкой тот текст, который им наиболее интересен(content_script)
### 2. Граф переходов между web-страницами позволяет определить наиболее популярные направления, по которым ходит пользователь, веса ребер графа будут показывать вероятность перехода по ребру, таким образом, можно отследить если вдруг злоумышленник захочет поменять пароль или персональные данные в соц сетях и тому подобное.
### 3. Клиент серверная часть. На сервере будет работать python скрипт, который будет распараллеливать поступления информации. Клиент и сервер будет писать в основном Максим.
### 4. База данных. 
* База данных мы планируем реализовать на сервере и все вычисления соответственно производиться будут на нем же. В базе пока что есть три таблицы:
* * Таблица пользователей "users" с колонками:
* * * id (Primary key)
* * * name
* * Таблица web-страниц "webpage" с колонками:
* * * id (Primary key)
* * * user_id (Foreign key (user_id) references users(id))
* * * model (параметры модели, построенной для данного пользователя и данной страницы)
* * * url
* * Таблица данных "data" с колонками:
* * * id (Primary key)
* * * webpage_id (Foreign key (webpage_id) references webpage(id))
* * * data (это целое множество колонок, они будут определены чуть позже)
* Также на базу будет наложен триггер, который будет срабатывать при появлении достаточного количества новых данных в таблице data. Этот триггер будет запускать python скрипт, подставляющий в модель новые полученные значения и считающий среднюю точность соответствия данные модели, если эта точность окажется ниже некоторого порога, то сервер передает клиенту сообщение о том, что следует запросить у пользователя пароль, подтверждающий личность.  

### 5. Для понимания структуры данных необходимо для начала визуализировать то, что получено от браузера. Кроме обычных методов регрессии и классификации планируется посмотреть на набор координат мыши, как на изображение. Если разбить всю web-страницу на некоторые области, можно подсчитать с какой вероятностью мышка пользователя находится в той или иной части, таким образом данная задача аналогична задаче об анализе изображений, где изображение - массив цветов. 
### 6. Мы будем запоминать пути, пройденные пользователем в течении некоторого периода и, умножая веса каждого из ребер по правилу произведения вероятностей, будем находить общую вероятность пройти по данному пути. Если эта вероятность окажется ниже пороговой, то за компьютером потенциальный злоумышленник.



